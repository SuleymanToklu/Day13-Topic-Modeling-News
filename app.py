import streamlit as st
import joblib
import pandas as pd

st.set_page_config(
    page_title="Konu KeÅŸfi: Ã–rÃ¼mcek AÄŸÄ±ndaki Kelimeler",
    page_icon="ğŸ•¸ï¸",
    layout="wide",
    initial_sidebar_state="collapsed"
)

# CSS 
# "Ã–rÃ¼mcek AÄŸÄ±" ve "Yapay Zeka" temasÄ±na uygun karanlÄ±k ve fÃ¼tÃ¼ristik bir stil
CUSTOM_CSS = """
<style>
    /* Ana arkaplan ve metin renkleri */
    .stApp {
        background-color: #0a0a1a;
        color: #e0e0e0;
    }
    
    /* BaÅŸlÄ±k stilleri */
    h1, h2, h3 {
        color: #00bfff; /* Deep Sky Blue */
        text-shadow: 2px 2px 4px #000000;
    }

    /* Tab stilleri */
    .stTabs [data-baseweb="tab-list"] {
        gap: 24px;
    }
    .stTabs [data-baseweb="tab"] {
        height: 50px;
        white-space: pre-wrap;
        background-color: #1a1a2e;
        border-radius: 4px 4px 0px 0px;
        gap: 1px;
        padding-top: 10px;
        padding-bottom: 10px;
        color: #a0a0a0;
    }
    .stTabs [aria-selected="true"] {
        background-color: #00bfff;
        color: white;
        font-weight: bold;
    }
    
    /* Konu kartlarÄ± iÃ§in Ã¶zel stil */
    .topic-card {
        background-color: #1a1a2e;
        border: 1px solid #00bfff;
        border-radius: 10px;
        padding: 20px;
        margin-bottom: 20px;
        box-shadow: 0 4px 8px 0 rgba(0, 191, 255, 0.2);
        transition: 0.3s;
        height: 100%;
    }
    .topic-card:hover {
        box-shadow: 0 8px 16px 0 rgba(0, 191, 255, 0.4);
    }
    .topic-title {
        font-size: 1.5em;
        color: #ffffff;
        border-bottom: 2px solid #00bfff;
        padding-bottom: 10px;
        margin-bottom: 15px;
    }
    .topic-words {
        font-size: 1.1em;
        color: #c0c0c0;
        line-height: 1.6;
    }
    
    /* Expander stilleri */
    .st-expander {
        border: 1px solid #333;
        border-radius: 5px;
    }
    .st-expander header {
        font-size: 1.2em;
        font-weight: bold;
        color: #00bfff;
    }
</style>
"""
st.markdown(CUSTOM_CSS, unsafe_allow_html=True)


# KaynaklarÄ± YÃ¼kleme 
@st.cache_resource
def load_resources():
    """LDA modelini ve vektÃ¶rleyiciyi diskten yÃ¼kler."""
    try:
        lda_model = joblib.load('lda_model.pkl')
        vectorizer = joblib.load('vectorizer.pkl')
        return lda_model, vectorizer
    except FileNotFoundError:
        return None, None

lda_model, vectorizer = load_resources()

# Ana BaÅŸlÄ±k ve GiriÅŸ 
st.title("ğŸ•¸ï¸ Konu KeÅŸfi: Haber BaÅŸlÄ±klarÄ± AÄŸÄ±ndaki Gizli Desenler")
st.markdown("""
Bu proje, bir **GÃ¶zetimsiz Ã–ÄŸrenme** modeli olan **LDA (Latent Dirichlet Allocation)** kullanarak binlerce haber baÅŸlÄ±ÄŸÄ±nÄ±n ardÄ±ndaki gizli "konularÄ±" ortaya Ã§Ä±karÄ±yor. 
TÄ±pkÄ± bir Ã¶rÃ¼mceÄŸin aÄŸÄ±nÄ± Ã¶rerek farklÄ± noktalarÄ± birleÅŸtirmesi gibi, bu yapay zeka da kelimeler arasÄ±ndaki gÃ¶rÃ¼nmez baÄŸlantÄ±larÄ± bularak anlamlÄ± konu gruplarÄ± oluÅŸturuyor.
""")

# Hata KontrolÃ¼ 
if lda_model is None or vectorizer is None:
    st.error(
        "Gerekli model dosyalarÄ± (.pkl) bulunamadÄ±. "
        "LÃ¼tfen Ã¶nce `python process_data.py` komutunu Ã§alÄ±ÅŸtÄ±rdÄ±ÄŸÄ±nÄ±zdan emin olun."
    )
    st.stop()

# Sekmeli Ä°Ã§erik YapÄ±sÄ± 
tab1, tab2 = st.tabs(["**ğŸ•·ï¸ KEÅFEDÄ°LEN KONU AÄLARI**", "**ğŸ§  MODELÄ°N BEYNÄ°: DERÄ°NLEMESÄ°NE ANALÄ°Z**"])

#  Helper Function for Topic Card 
def create_topic_card(topic_num, words):
    """HTML ve CSS kullanarak ÅŸÄ±k bir konu kartÄ± oluÅŸturur."""
    words_html = f"<span class='topic-words'>{', '.join(words)}</span>"
    card_html = f"""
    <div class="topic-card">
        <h3 class="topic-title">Konu AÄŸÄ± #{topic_num}</h3>
        {words_html}
    </div>
    """
    return card_html

with tab1:
    st.header("Yapay ZekanÄ±n Ã–rdÃ¼ÄŸÃ¼ 10 Anlam AÄŸÄ±")
    st.write(
        "LDA modeli, 20,000 haber baÅŸlÄ±ÄŸÄ±nÄ± analiz ederek aÅŸaÄŸÄ±daki 10 gizli konuyu ve bu konularÄ± en iyi tanÄ±mlayan "
        "anahtar kelimeleri tamamen kendi baÅŸÄ±na, hiÃ§bir insan mÃ¼dahalesi olmadan buldu. Her bir kart, kelimelerin bir araya gelerek oluÅŸturduÄŸu bir 'anlam aÄŸÄ±nÄ±' temsil ediyor."
    )
    st.markdown("---")

    feature_names = vectorizer.get_feature_names_out()
    
    col1, col2, col3, col4, col5 = st.columns(5)
    columns = [col1, col2, col3, col4, col5]

    for topic_idx, topic in enumerate(lda_model.components_):
        top_words_idx = topic.argsort()[:-10 - 1:-1]
        top_words = [feature_names[i] for i in top_words_idx]
        
        with columns[topic_idx % 5]:
            card = create_topic_card(topic_idx + 1, top_words)
            st.markdown(card, unsafe_allow_html=True)


with tab2:
    st.header("Projenin ve Modelin ArdÄ±ndaki MantÄ±k")

    st.markdown("""
    Bu bÃ¶lÃ¼mde, projenin "nasÄ±l" Ã§alÄ±ÅŸtÄ±ÄŸÄ±nÄ±, GÃ¶zetimsiz Ã–ÄŸrenme'nin ne anlama geldiÄŸini ve LDA modelinin kelime yÄ±ÄŸÄ±nlarÄ±ndan nasÄ±l anlamlÄ± konular Ã§Ä±kardÄ±ÄŸÄ±nÄ± keÅŸfedeceÄŸiz.
    """)

    with st.expander("ğŸ“– Projenin Hikayesi: Kaos Ä°Ã§indeki DÃ¼zen ArayÄ±ÅŸÄ±"):
        st.markdown("""
        **Problem:** Her gÃ¼n milyonlarca haber Ã¼retiliyor. Bu devasa bilgi okyanusunda hangi konularÄ±n popÃ¼ler olduÄŸunu, hangi olaylarÄ±n birbiriyle iliÅŸkili olduÄŸunu manuel olarak takip etmek imkansÄ±z. Elimizde sadece etiketlenmemiÅŸ binlerce baÅŸlÄ±k var.
        
        **AmaÃ§:** Bu kaotik veri yÄ±ÄŸÄ±nÄ±na bir dÃ¼zen getirmek. BaÅŸlÄ±klarÄ±, iÃ§erdikleri ortak anlamlara gÃ¶re otomatik olarak gruplayan bir sistem kurmak. Yani, "kaosun iÃ§indeki gizli dÃ¼zeni" bulmak. Ä°ÅŸte GÃ¶zetimsiz Ã–ÄŸrenme burada devreye giriyor.
        """)

    with st.expander("ğŸ¤” GÃ¶zetimsiz Ã–ÄŸrenme Nedir? (KÃ¼tÃ¼phaneci Analojisi)"):
        st.markdown("""
        GÃ¶zetimsiz Ã–ÄŸrenme'yi anlamanÄ±n en kolay yolu **kÃ¼tÃ¼phaneci analojisidir**:
        
        Imagine a library receives thousands of books with no titles, no covers, and no category labels. A supervised learning librarian would need someone to read each book and tell them "this is a science fiction book," "this is a history book."
        
        An **unsupervised** learning librarian, on the other hand, is given no such help. They must figure out the categories on their own. How?
        
        1.  They start looking for patterns. They notice some books frequently use words like "galaxy," "spaceship," and "alien." They put these into a pile.
        2.  They find another group of books that mention "king," "battle," and "castle" a lot. That's another pile.
        3.  A third pile contains words like "economy," "market," and "shares."
        
        At the end of the day, the librarian has created piles (topics) without ever being told what the topics were. They *inferred* the topics from the words inside the books.
        
        **Bizim projemiz de tam olarak bunu yapÄ±yor: Haber baÅŸlÄ±klarÄ± kitaplar, LDA modeli ise bu zeki kÃ¼tÃ¼phaneci.**
        """)

    with st.expander("ğŸ”® LDA Modelinin SÄ±rrÄ±: Bir 'Tarif' Olarak Konular"):
        st.markdown(r"""
        **Latent Dirichlet Allocation (LDA)**, ilk bakÄ±ÅŸta karmaÅŸÄ±k gelse de temelinde basit ve gÃ¼Ã§lÃ¼ bir fikre dayanÄ±r:
        
        > 1.  Her **belge (haber baÅŸlÄ±ÄŸÄ±)**, birden Ã§ok **konunun bir karÄ±ÅŸÄ±mÄ±dÄ±r**.
        > 2.  Her **konu**, belirli **kelimelerin bir karÄ±ÅŸÄ±mÄ±dÄ±r** (olasÄ±lÄ±k daÄŸÄ±lÄ±mÄ±dÄ±r).
        
        Bunu bir **smoothie tarifi** gibi dÃ¼ÅŸÃ¼nebiliriz:
        
        -   **Belge (Smoothie):** "Ã‡ilekli Muzlu Yaz RÃ¼yasÄ±" smoothiemiz, `%60 Ã‡ilek Lezzeti`, `%30 Muz Lezzeti` ve `%10 Vanilya Esintisi`'nin bir karÄ±ÅŸÄ±mÄ± olabilir.
        -   **Konu (Lezzet):** "Ã‡ilek Lezzeti" konusu ise `%50 'Ã§ilek'`, `%20 'ÅŸeker'`, `%15 'taze'`, `%10 'kÄ±rmÄ±zÄ±'` gibi kelimelerin bir karÄ±ÅŸÄ±mÄ±ndan oluÅŸur.
        
        LDA'nÄ±n yaptÄ±ÄŸÄ± sihir, bize hem smoothielerin iÃ§indeki lezzet oranlarÄ±nÄ± ($P(\text{konu } | \text{ belge})$) hem de her bir lezzeti oluÅŸturan kelime tariflerini ($P(\text{kelime } | \text{ konu})$) aynÄ± anda sunmasÄ±dÄ±r. Bunu, kelimelerin belgeler arasÄ±nda nasÄ±l daÄŸÄ±ldÄ±ÄŸÄ±nÄ± istatistiksel olarak analiz ederek yapar.
        """)

    with st.expander("âš™ï¸ AdÄ±m AdÄ±m Bizim Pipeline'Ä±mÄ±z"):
        st.markdown("""
        Peki bu teoriyi pratiÄŸe nasÄ±l dÃ¶ktÃ¼k? `process_data.py` script'i ÅŸu adÄ±mlarÄ± izledi:
        
        1.  **Veri YÃ¼kleme:** `abcnews-date-text.csv` dosyasÄ±ndan 20,000 adet rastgele haber baÅŸlÄ±ÄŸÄ± seÃ§tik.
        2.  **Metin Temizleme ve VektÃ¶rleÅŸtirme (`CountVectorizer`):**
            -   Metinleri sayÄ±lara dÃ¶nÃ¼ÅŸtÃ¼rmemiz gerekiyordu. `CountVectorizer` her bir baÅŸlÄ±ÄŸÄ± aldÄ±.
            -   'the', 'a', 'is' gibi yaygÄ±n Ä°ngilizce kelimeleri (`stop_words`) attÄ±.
            -   Her kelimenin her baÅŸlÄ±kta kaÃ§ kez geÃ§tiÄŸini sayarak dev bir matris oluÅŸturdu. Bu matris, modelimizin "okuyabildiÄŸi" formattÄ±r.
        3.  **Model EÄŸitimi (`LatentDirichletAllocation`):**
            -   Bu sayÄ±sal matrisi LDA modeline verdik ve ona "Bu verinin iÃ§inde 10 tane gizli konu bul" dedik.
            -   Model, kelimelerin hangi konularda ve konularÄ±n hangi belgelerde bir araya gelme olasÄ±lÄ±ÄŸÄ±nÄ±n en yÃ¼ksek olduÄŸunu hesaplayarak "konu aÄŸlarÄ±nÄ±" Ã¶rdÃ¼.
        4.  **Modeli Kaydetme (`joblib.dump`):**
            -   EÄŸittiÄŸimiz bu zeki "kÃ¼tÃ¼phaneciyi" (LDA modeli) ve kelime sÃ¶zlÃ¼ÄŸÃ¼nÃ¼ (Vectorizer) daha sonra bu uygulamada kullanmak Ã¼zere `.pkl` dosyalarÄ± olarak kaydettik.
        """)